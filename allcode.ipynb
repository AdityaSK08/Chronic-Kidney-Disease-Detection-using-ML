{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86de2c8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Data\n",
    "dataset = pd.read_csv(\"Kidney_data.csv\")\n",
    "dataset = dataset.drop('id', axis=1)\n",
    "dataset.dtypes\n",
    "dataset['rbc'] = dataset['rbc'].replace(to_replace = {'normal' : 0, 'abnormal' : 1})\n",
    "dataset['pc'] = dataset['pc'].replace(to_replace = {'normal' : 0, 'abnormal' : 1})\n",
    "dataset['pcc'] = dataset['pcc'].replace(to_replace = {'notpresent':0,'present':1})\n",
    "dataset['ba'] = dataset['ba'].replace(to_replace = {'notpresent':0,'present':1})\n",
    "dataset['htn'] = dataset['htn'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "dataset['dm'] = dataset['dm'].replace(to_replace = {'\\tyes':'yes', ' yes':'yes', '\\tno':'no'})\n",
    "dataset['dm'] = dataset['dm'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "dataset['cad'] = dataset['cad'].replace(to_replace = {'\\tno':'no'})\n",
    "dataset['cad'] = dataset['cad'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "\n",
    "dataset['appet'] = dataset['appet'].replace(to_replace={'good':1,'poor':0,'no':np.nan})\n",
    "\n",
    "dataset['pe'] = dataset['pe'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "\n",
    "dataset['ane'] = dataset['ane'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "\n",
    "dataset['classification'] = dataset['classification'].replace(to_replace={'ckd\\t':'ckd'})\n",
    "dataset[\"classification\"] = [1 if i == \"ckd\" else 0 for i in dataset[\"classification\"]]\n",
    "\n",
    "dataset.dtypes\n",
    "\n",
    "\n",
    "dataset['pcv'] = pd.to_numeric(dataset['pcv'], errors='coerce')\n",
    "dataset['wc'] = pd.to_numeric(dataset['wc'], errors='coerce')\n",
    "dataset['rc'] = pd.to_numeric(dataset['rc'], errors='coerce')\n",
    "dataset.dtypes\n",
    "\n",
    "for feature in dataset.columns:\n",
    "    dataset[feature] = dataset[feature].fillna(dataset[feature].median())\n",
    "X = dataset.drop('classification', axis=1)\n",
    "y = dataset['classification']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "### RANDOM FOREST CLASSIFIER ###\n",
    "\n",
    "# Baseline Model with fewer estimators and less informative features\n",
    "rf_baseline = RandomForestClassifier(n_estimators=5, random_state=42)  # Few trees\n",
    "rf_baseline.fit(X_train[['sg', 'pc']], y_train)  # Using less informative features\n",
    "y_pred_rf_baseline = rf_baseline.predict(X_test[['sg', 'pc']])\n",
    "\n",
    "# Baseline Random Forest Model Accuracy and Confusion Matrix\n",
    "rf_baseline_accuracy = accuracy_score(y_test, y_pred_rf_baseline)\n",
    "print(f\"Random Forest Baseline Model Accuracy: {rf_baseline_accuracy:.2f}\")\n",
    "print(\"Random Forest Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf_baseline))\n",
    "# Plot Confusion Matrix for Baseline Random Forest Model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_rf_baseline), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Random Forest Baseline Model Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "### ADA BOOST CLASSIFIER ###\n",
    "\n",
    "# Baseline Model with fewer estimators\n",
    "ada_baseline = AdaBoostClassifier(n_estimators=5, random_state=42)  # Few trees\n",
    "ada_baseline.fit(X_train[['sg', 'pc']], y_train)\n",
    "y_pred_ada_baseline = ada_baseline.predict(X_test[['sg', 'pc']])\n",
    "\n",
    "# Baseline AdaBoost Model Accuracy and Confusion Matrix\n",
    "ada_baseline_accuracy = accuracy_score(y_test, y_pred_ada_baseline)\n",
    "print(f\"AdaBoost Baseline Model Accuracy: {ada_baseline_accuracy:.2f}\")\n",
    "print(\"AdaBoost Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ada_baseline))\n",
    "# Plot Confusion Matrix for Baseline AdaBoost Model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_ada_baseline), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('AdaBoost Baseline Model Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "### GRADIENT BOOSTING CLASSIFIER ###\n",
    "\n",
    "# Baseline Model with fewer estimators\n",
    "gb_baseline = GradientBoostingClassifier(n_estimators=5, random_state=42)  # Few trees\n",
    "gb_baseline.fit(X_train[['sg', 'pc']], y_train)\n",
    "y_pred_gb_baseline = gb_baseline.predict(X_test[['sg', 'pc']])\n",
    "\n",
    "# Baseline Gradient Boosting Model Accuracy and Confusion Matrix\n",
    "gb_baseline_accuracy = accuracy_score(y_test, y_pred_gb_baseline)\n",
    "print(f\"Gradient Boosting Baseline Model Accuracy: {gb_baseline_accuracy:.2f}\")\n",
    "print(\"Gradient Boosting Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb_baseline))\n",
    "\n",
    "# Plot Confusion Matrix for Baseline Gradient Boosting Model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_gb_baseline), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Gradient Boosting Baseline Model Confusion Matrix')\n",
    "plt.show()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Baseline Model (SVM without feature selection or scaling)\n",
    "svm_baseline = SVC(random_state=42)\n",
    "svm_baseline.fit(X_train, y_train)\n",
    "y_pred_svm_baseline = svm_baseline.predict(X_test)\n",
    "\n",
    "# Baseline SVM Model Accuracy and Confusion Matrix\n",
    "svm_baseline_accuracy = accuracy_score(y_test, y_pred_svm_baseline)\n",
    "print(f\"SVM Baseline Model Accuracy: {svm_baseline_accuracy:.2f}\")\n",
    "print(\"SVM Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm_baseline))\n",
    "# Plot Confusion Matrix for Baseline Gradient Boosting Model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_gb_baseline), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Gradient Boosting Baseline Model Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### K-NEAREST NEIGHBORS (KNN) ###\n",
    "\n",
    "# Baseline Model (KNN without feature selection or scaling)\n",
    "knn_baseline = KNeighborsClassifier()\n",
    "knn_baseline.fit(X_train, y_train)\n",
    "y_pred_knn_baseline = knn_baseline.predict(X_test)\n",
    "\n",
    "# Baseline KNN Model Accuracy and Confusion Matrix\n",
    "knn_baseline_accuracy = accuracy_score(y_test, y_pred_knn_baseline)\n",
    "print(f\"KNN Baseline Model Accuracy: {knn_baseline_accuracy:.2f}\")\n",
    "print(\"KNN Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_knn_baseline))\n",
    "\n",
    "# Plot Confusion Matrix for Baseline KNN Model\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_knn_baseline), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('KNN Baseline Model Confusion Matrix')\n",
    "plt.show()\n",
    "# Comparison of performance across baseline models only\n",
    "plt.figure(figsize=(10, 6))\n",
    "models_baseline = ['RF Baseline', 'GB Baseline', 'SVM Baseline', 'KNN Baseline', 'AdaBoost Baseline']\n",
    "accuracies_baseline = [rf_baseline_accuracy, gb_baseline_accuracy, svm_baseline_accuracy, knn_baseline_accuracy, ada_baseline_accuracy]\n",
    "\n",
    "sns.barplot(x=models_baseline, y=accuracies_baseline, palette='Set2')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Accuracy Comparison Across Baseline Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Importing Libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# for displaying all feature from dataset:\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "# Reading Dataset:\n",
    "dataset = pd.read_csv(\"Kidney_data.csv\")\n",
    "# Top 5 records:\n",
    "dataset.head()\n",
    "# Dropping unneccsary feature :\n",
    "dataset = dataset.drop('id', axis=1)\n",
    "# Shape of dataset:\n",
    "dataset.shape\n",
    "# Cheaking Missing (NaN) Values:\n",
    "dataset.isnull().sum()\n",
    "\n",
    "# Description:\n",
    "dataset.describe()\n",
    "# Datatypes:\n",
    "dataset.dtypes\n",
    "dataset.head()\n",
    "dataset['rbc'].value_counts()\n",
    "dataset['rbc'] = dataset['rbc'].replace(to_replace = {'normal' : 0, 'abnormal' : 1})\n",
    "dataset['pc'].value_counts()\n",
    "dataset['pc'] = dataset['pc'].replace(to_replace = {'normal' : 0, 'abnormal' : 1})\n",
    "\n",
    "\n",
    "dataset['pcc'].value_counts()\n",
    "data=dataset['pcc']\n",
    "data\n",
    "dataset['pcc'] = dataset['pcc'].replace(to_replace = {'notpresent':0,'present':1})\n",
    "dataset['ba'].value_counts()\n",
    "\n",
    "dataset['ba'] = dataset['ba'].replace(to_replace = {'notpresent':0,'present':1})\n",
    "data=dataset['ba']\n",
    "data\n",
    "dataset['htn'].value_counts()\n",
    "dataset['htn'] = dataset['htn'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "data=dataset['htn']\n",
    "data\n",
    "dataset['dm'].value_counts()\n",
    "\n",
    "dataset['dm'] = dataset['dm'].replace(to_replace = {'\\tyes':'yes', ' yes':'yes', '\\tno':'no'})\n",
    "\n",
    "dataset['dm'] = dataset['dm'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "\n",
    "dataset['cad'].value_counts()\n",
    "\n",
    "dataset['cad'] = dataset['cad'].replace(to_replace = {'\\tno':'no'})\n",
    "\n",
    "dataset['cad'] = dataset['cad'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "\n",
    "dataset['appet'].unique()\n",
    "dataset['appet'] = dataset['appet'].replace(to_replace={'good':1,'poor':0,'no':np.nan})\n",
    "dataset['pe'].value_counts()\n",
    "dataset['pe'] = dataset['pe'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "dataset['ane'].value_counts()\n",
    "dataset['ane'] = dataset['ane'].replace(to_replace = {'yes' : 1, 'no' : 0})\n",
    "dataset['classification'].value_counts()\n",
    "dataset['classification'] = dataset['classification'].replace(to_replace={'ckd\\t':'ckd'})\n",
    "dataset[\"classification\"] = [1 if i == \"ckd\" else 0 for i in dataset[\"classification\"]]\n",
    "dataset.head()\n",
    "dataset.dtypes\n",
    "dataset['pcv'] = pd.to_numeric(dataset['pcv'], errors='coerce')\n",
    "dataset['wc'] = pd.to_numeric(dataset['wc'], errors='coerce')\n",
    "dataset['rc'] = pd.to_numeric(dataset['rc'], errors='coerce')\n",
    "dataset.dtypes\n",
    "dataset.describe()\n",
    "# Cheaking Missing (NaN) Values:\n",
    "dataset.isnull().sum().sort_values(ascending=False)\n",
    "dataset.columns\n",
    "features = ['age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr', 'bu',\n",
    "           'sc', 'sod', 'pot', 'hemo', 'pcv', 'wc', 'rc', 'htn', 'dm', 'cad',\n",
    "           'appet', 'pe', 'ane']\n",
    "for feature in features:\n",
    "    dataset[feature] = dataset[feature].fillna(dataset[feature].median())\n",
    "dataset.isnull().any().sum()\n",
    "plt.figure(figsize=(24,14))\n",
    "sns.heatmap(dataset.corr(), annot=True, cmap='YlGnBu')\n",
    "plt.show()\n",
    "dataset.drop('pcv', axis=1, inplace=True)\n",
    "dataset.head()\n",
    "sns.countplot(dataset['classification'])\n",
    "# Independent and Dependent Feature:\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]\n",
    "X.head()\n",
    "# Feature Importance:\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model=ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "ranked_features=pd.Series(model.feature_importances_,index=X.columns)\n",
    "ranked_features.nlargest(24).plot(kind='barh')\n",
    "plt.show()\n",
    "ranked_features.nlargest(8).index\n",
    "\n",
    "X = dataset[['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']]\n",
    "X.head()\n",
    "X.tail()\n",
    "y.head()\n",
    "# Train Test Split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=33)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# Importing Performance Metrics:\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# RandomForestClassifier:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForest = RandomForestClassifier()\n",
    "RandomForest = RandomForest.fit(X_train,y_train)\n",
    "\n",
    "# Predictions:\n",
    "y_pred = RandomForest.predict(X_test)\n",
    "\n",
    "# Performance:\n",
    "print('Accuracy:', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# AdaBoostClassifier:\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "AdaBoost = AdaBoostClassifier()\n",
    "AdaBoost = AdaBoost.fit(X_train,y_train)\n",
    "\n",
    "# Predictions:\n",
    "y_pred = AdaBoost.predict(X_test)\n",
    "\n",
    "# Performance:\n",
    "print('Accuracy:', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "# GradientBoostingClassifier:\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GradientBoost = GradientBoostingClassifier()\n",
    "GradientBoost = GradientBoost.fit(X_train,y_train)\n",
    "\n",
    "# Predictions:\n",
    "y_pred = GradientBoost.predict(X_test)\n",
    "\n",
    "# Performance:\n",
    "print('Accuracy:', accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "from sklearn.svm import SVC\n",
    "SVM = SVC(kernel='linear')  \n",
    "SVM = SVM.fit(X_train, y_train)\n",
    "\n",
    "# Predictions:\n",
    "y_pred = SVM.predict(X_test)\n",
    "\n",
    "# Performance:\n",
    "print('SVM Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # KNN Classifier:\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# KNN = KNeighborsClassifier(n_neighbors=5)  \n",
    "# KNN = KNN.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions:\n",
    "# y_pred = KNN.predict(X_test)\n",
    "\n",
    "# # Performance:\n",
    "# print('KNN Accuracy:', accuracy_score(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Collecting accuracies of all models\n",
    "# models = ['RandomForest', 'AdaBoost', 'GradientBoost', 'SVM', 'KNN']\n",
    "# accuracies = [accuracy_score(y_test, RandomForest.predict(X_test)),\n",
    "#               accuracy_score(y_test, AdaBoost.predict(X_test)),\n",
    "#               accuracy_score(y_test, GradientBoost.predict(X_test)),\n",
    "#               accuracy_score(y_test, SVM.predict(X_test)),\n",
    "#               accuracy_score(y_test, KNN.predict(X_test))]\n",
    "\n",
    "# # Plotting the results\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.barplot(x=models, y=accuracies, palette=\"viridis\")\n",
    "# plt.title('Comparison of Model Accuracies')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "# print(f\"Baseline Model Accuracy 1: {baseline_accuracy:.2f}\")\n",
    "# print(\"Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_baseline))\n",
    "\n",
    "\n",
    "# selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train[selected_features])\n",
    "# X_test_scaled = scaler.transform(X_test[selected_features])\n",
    "\n",
    "# # RandomForest Classifier with optimized features and scaling\n",
    "# rf_optimized = RandomForestClassifier(random_state=42)\n",
    "# rf_optimized.fit(X_train_scaled, y_train)\n",
    "# y_pred_optimized = rf_optimized.predict(X_test_scaled)\n",
    "# optimized_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "# print(f\"Optimized Model Accuracy: {optimized_accuracy:.2f}\")\n",
    "# print(\"Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_optimized))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Assuming X and y are already defined and are your feature matrix and target variable\n",
    "# If not, replace this with the correct dataset loading method.\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# RandomForest Classifier as a baseline\n",
    "baseline_model = RandomForestClassifier(random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions for the baseline model\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "\n",
    "# Baseline model performance\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Model Accuracy: {baseline_accuracy:.2f}\")\n",
    "print(\"Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_baseline))\n",
    "print(\"Baseline Classification Report:\\n\", classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "### OPTIMIZED FEATURES & SCALING ###\n",
    "\n",
    "# Feature selection\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[selected_features])\n",
    "X_test_scaled = scaler.transform(X_test[selected_features])\n",
    "\n",
    "# RandomForest Classifier with optimized features and scaling\n",
    "rf_optimized = RandomForestClassifier(random_state=42)\n",
    "rf_optimized.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions for the optimized model\n",
    "y_pred_optimized = rf_optimized.predict(X_test_scaled)\n",
    "\n",
    "# Optimized model performance\n",
    "optimized_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "print(f\"Optimized Model Accuracy: {optimized_accuracy:.2f}\")\n",
    "print(\"Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_optimized))\n",
    "print(\"Optimized Classification Report:\\n\", classification_report(y_test, y_pred_optimized))\n",
    "\n",
    "### OTHER MODELS ###\n",
    "\n",
    "# Instantiate and train models\n",
    "AdaBoost = AdaBoostClassifier(random_state=42)\n",
    "GradientBoost = GradientBoostingClassifier(random_state=42)\n",
    "SVM = SVC(kernel='linear', random_state=42)\n",
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit models\n",
    "AdaBoost.fit(X_train, y_train)\n",
    "GradientBoost.fit(X_train, y_train)\n",
    "SVM.fit(X_train, y_train)\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# Predictions for each model\n",
    "y_pred_adaboost = AdaBoost.predict(X_test)\n",
    "y_pred_gradientboost = GradientBoost.predict(X_test)\n",
    "y_pred_svm = SVM.predict(X_test)\n",
    "y_pred_knn = KNN.predict(X_test)\n",
    "\n",
    "# Collecting accuracies of all models\n",
    "models = ['RandomForest', 'AdaBoost', 'GradientBoost', 'SVM', 'KNN']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test, y_pred_baseline),\n",
    "    accuracy_score(y_test, y_pred_adaboost),\n",
    "    accuracy_score(y_test, y_pred_gradientboost),\n",
    "    accuracy_score(y_test, y_pred_svm),\n",
    "    accuracy_score(y_test, y_pred_knn)\n",
    "]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=models, y=accuracies, palette=\"viridis\")\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Split dataset for baseline model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
    "\n",
    "# Baseline RandomForest model\n",
    "rf_baseline = RandomForestClassifier(random_state=42)\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_baseline = rf_baseline.predict(X_test)\n",
    "\n",
    "# Baseline performance\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Model Accuracy: {baseline_accuracy:.2f}\")\n",
    "print(\"Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_baseline))\n",
    "print(\"Baseline Classification Report:\\n\", classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "\n",
    "# Split dataset for optimized model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale selected features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[selected_features])\n",
    "X_test_scaled = scaler.transform(X_test[selected_features])\n",
    "\n",
    "# Optimized RandomForest model\n",
    "rf_optimized = RandomForestClassifier(random_state=42)\n",
    "rf_optimized.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Optimized predictions\n",
    "y_pred_optimized = rf_optimized.predict(X_test_scaled)\n",
    "\n",
    "# Optimized performance\n",
    "optimized_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "print(f\"Optimized Model Accuracy: {optimized_accuracy:.2f}\")\n",
    "print(\"Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_optimized))\n",
    "print(\"Optimized Classification Report:\\n\", classification_report(y_test, y_pred_optimized))\n",
    "\n",
    "### Plotting the Accuracy Comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['Baseline Model', 'Optimized Model']\n",
    "accuracies = [baseline_accuracy, optimized_accuracy]\n",
    "\n",
    "# Plotting the bar chart\n",
    "sns.barplot(x=models, y=accuracies, palette='viridis')\n",
    "\n",
    "# Adding accuracy labels on top of the bars\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    plt.text(i, accuracy + 0.001, f'{accuracy:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Setting y-axis limits for better differentiation\n",
    "plt.ylim(0.9, 1.05)  # Adjust this range as necessary\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Accuracy Comparison: Baseline vs Optimized RandomForest')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model Type')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Split dataset for baseline model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# Baseline Gradient Boosting model\n",
    "gb_baseline = GradientBoostingClassifier(random_state=42)\n",
    "gb_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_gb_baseline = gb_baseline.predict(X_test)\n",
    "\n",
    "# Baseline performance\n",
    "gb_baseline_accuracy = accuracy_score(y_test, y_pred_gb_baseline)\n",
    "print(f\"Gradient Boosting Baseline Model Accuracy: {gb_baseline_accuracy:.2f}\")\n",
    "print(\"Gradient Boosting Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb_baseline))\n",
    "print(\"Gradient Boosting Baseline Classification Report:\\n\", classification_report(y_test, y_pred_gb_baseline))\n",
    "\n",
    "### OPTIMIZED MODEL ###\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "\n",
    "# Split dataset for optimized model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale selected features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[selected_features])\n",
    "X_test_scaled = scaler.transform(X_test[selected_features])\n",
    "\n",
    "# Optimized Gradient Boosting model\n",
    "gb_optimized = GradientBoostingClassifier(random_state=42)\n",
    "gb_optimized.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Optimized predictions\n",
    "y_pred_gb_optimized = gb_optimized.predict(X_test_scaled)\n",
    "\n",
    "# Optimized performance\n",
    "gb_optimized_accuracy = accuracy_score(y_test, y_pred_gb_optimized)\n",
    "print(f\"Gradient Boosting Optimized Model Accuracy: {gb_optimized_accuracy:.2f}\")\n",
    "print(\"Gradient Boosting Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb_optimized))\n",
    "print(\"Gradient Boosting Optimized Classification Report:\\n\", classification_report(y_test, y_pred_gb_optimized))\n",
    "\n",
    "### Plotting the Accuracy Comparison ###\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['Gradient Boosting Baseline', 'Gradient Boosting Optimized']\n",
    "accuracies = [gb_baseline_accuracy, gb_optimized_accuracy]\n",
    "\n",
    "# Plotting the bar chart\n",
    "sns.barplot(x=models, y=accuracies, palette='viridis')\n",
    "\n",
    "# Adding accuracy labels on top of the bars\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    plt.text(i, accuracy + 0.001, f'{accuracy:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Setting y-axis limits for better differentiation\n",
    "plt.ylim(0.9, 1.05)  # Adjust this range as necessary\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Accuracy Comparison: Baseline vs Optimized Gradient Boosting')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model Type')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Split dataset for baseline model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# Baseline AdaBoost model\n",
    "ada_baseline = AdaBoostClassifier(n_estimators=50, learning_rate=0.5, random_state=42)  # Lower learning rate\n",
    "ada_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_ada_baseline = ada_baseline.predict(X_test)\n",
    "\n",
    "# Baseline performance\n",
    "ada_baseline_accuracy = accuracy_score(y_test, y_pred_ada_baseline)\n",
    "print(f\"AdaBoost Baseline Model Accuracy: {ada_baseline_accuracy:.2f}\")\n",
    "print(\"AdaBoost Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ada_baseline))\n",
    "print(\"AdaBoost Baseline Classification Report:\\n\", classification_report(y_test, y_pred_ada_baseline))\n",
    "\n",
    "### OPTIMIZED MODEL ###\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "\n",
    "# Split dataset for optimized model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale selected features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[selected_features])\n",
    "X_test_scaled = scaler.transform(X_test[selected_features])\n",
    "\n",
    "# Optimized AdaBoost model\n",
    "ada_optimized = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)  # Higher learning rate\n",
    "ada_optimized.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Optimized predictions\n",
    "y_pred_ada_optimized = ada_optimized.predict(X_test_scaled)\n",
    "\n",
    "# Optimized performance\n",
    "ada_optimized_accuracy = accuracy_score(y_test, y_pred_ada_optimized)\n",
    "print(f\"AdaBoost Optimized Model Accuracy: {ada_optimized_accuracy:.2f}\")\n",
    "print(\"AdaBoost Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ada_optimized))\n",
    "print(\"AdaBoost Optimized Classification Report:\\n\", classification_report(y_test, y_pred_ada_optimized))\n",
    "\n",
    "### Plotting the Accuracy Comparison ###\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['AdaBoost Baseline', 'AdaBoost Optimized']\n",
    "accuracies = [ada_baseline_accuracy, ada_optimized_accuracy]\n",
    "\n",
    "# Plotting the bar chart\n",
    "sns.barplot(x=models, y=accuracies, palette='viridis')\n",
    "\n",
    "# Adding accuracy labels on top of the bars\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    plt.text(i, accuracy + 0.001, f'{accuracy:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Setting y-axis limits for better differentiation\n",
    "plt.ylim(0.9, 1.05)  # Adjust this range as necessary\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Accuracy Comparison: Baseline vs Optimized AdaBoost')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model Type')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Split dataset for baseline model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# Baseline SVM model (linear kernel)\n",
    "SVM = SVC(kernel='linear', random_state=42)\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_svm_baseline = SVM.predict(X_test)\n",
    "\n",
    "# Baseline performance\n",
    "svm_baseline_accuracy = accuracy_score(y_test, y_pred_svm_baseline)\n",
    "print(f\"SVM Baseline Model Accuracy: {svm_baseline_accuracy:.2f}\")\n",
    "print(\"SVM Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm_baseline))\n",
    "print(\"SVM Baseline Classification Report:\\n\", classification_report(y_test, y_pred_svm_baseline))\n",
    "\n",
    "### OPTIMIZED MODEL ###\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "\n",
    "# Split dataset for optimized model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale selected features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[selected_features])\n",
    "X_test_scaled = scaler.transform(X_test[selected_features])\n",
    "\n",
    "# Optimized SVM model\n",
    "svm_optimized = SVC(random_state=42)\n",
    "svm_optimized.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Optimized predictions\n",
    "y_pred_svm_optimized = svm_optimized.predict(X_test_scaled)\n",
    "\n",
    "# Optimized performance\n",
    "svm_optimized_accuracy = accuracy_score(y_test, y_pred_svm_optimized)\n",
    "print(f\"SVM Optimized Model Accuracy: {svm_optimized_accuracy:.2f}\")\n",
    "print(\"SVM Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm_optimized))\n",
    "print(\"SVM Optimized Classification Report:\\n\", classification_report(y_test, y_pred_svm_optimized))\n",
    "\n",
    "### Plotting the Accuracy Comparison ###\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['SVM Baseline', 'SVM Optimized']\n",
    "accuracies = [svm_baseline_accuracy, svm_optimized_accuracy]\n",
    "\n",
    "# Plotting the bar chart\n",
    "sns.barplot(x=models, y=accuracies, palette='coolwarm')\n",
    "\n",
    "# Adding accuracy labels on top of the bars\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    plt.text(i, accuracy + 0.001, f'{accuracy:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Setting y-axis limits for better differentiation\n",
    "plt.ylim(0.9, 1.05)  # Adjust this range as necessary\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Accuracy Comparison: Baseline vs Optimized SVM')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model Type')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Baseline Model Performance (already provided in the previous step)\n",
    "knn_baseline = KNeighborsClassifier(n_neighbors=5)  \n",
    "knn_baseline.fit(X_train, y_train)\n",
    "y_pred_knn_baseline = knn_baseline.predict(X_test)\n",
    "\n",
    "# Baseline Accuracy\n",
    "knn_baseline_accuracy = accuracy_score(y_test, y_pred_knn_baseline)\n",
    "print(f\"KNN Baseline Model Accuracy: {knn_baseline_accuracy:.2f}\")\n",
    "print(\"Baseline Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_knn_baseline))\n",
    "print(\"Baseline Classification Report:\\n\", classification_report(y_test, y_pred_knn_baseline))\n",
    "\n",
    "# Optimized Model Performance\n",
    "knn_optimized = KNeighborsClassifier()\n",
    "knn_optimized.fit(X_train_scaled, y_train)\n",
    "y_pred_knn_optimized = knn_optimized.predict(X_test_scaled)\n",
    "\n",
    "# Optimized Accuracy\n",
    "knn_optimized_accuracy = accuracy_score(y_test, y_pred_knn_optimized)\n",
    "print(f\"KNN Optimized Model Accuracy: {knn_optimized_accuracy:.2f}\")\n",
    "print(\"Optimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_knn_optimized))\n",
    "print(\"Optimized Classification Report:\\n\", classification_report(y_test, y_pred_knn_optimized))\n",
    "\n",
    "# Plotting the accuracy comparison\n",
    "models = ['KNN Baseline', 'KNN Optimized']\n",
    "accuracies = [knn_baseline_accuracy, knn_optimized_accuracy]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, accuracies, color=['blue', 'green'])\n",
    "plt.title('KNN Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)  # Accuracy ranges from 0 to 1\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train Test Split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Initialize a list to store model names and accuracies\n",
    "models = []\n",
    "accuracies = []\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "models.append('Random Forest')\n",
    "accuracies.append(rf_accuracy)\n",
    "print('Random Forest Accuracy:', rf_accuracy)\n",
    "\n",
    "\n",
    "# AdaBoost Classifier\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "ada_accuracy = accuracy_score(y_test, y_pred_ada)\n",
    "models.append('AdaBoost')\n",
    "accuracies.append(ada_accuracy)\n",
    "print('AdaBoost Accuracy:', ada_accuracy)\n",
    "\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "models.append('Gradient Boosting')\n",
    "accuracies.append(gb_accuracy)\n",
    "print('Gradient Boosting Accuracy:', gb_accuracy)\n",
    "\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "models.append('SVM')\n",
    "accuracies.append(svm_accuracy)\n",
    "print('SVM Accuracy:', svm_accuracy)\n",
    "\n",
    "\n",
    "# K-Nearest Neighbors (KNN)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "models.append('KNN')\n",
    "accuracies.append(knn_accuracy)\n",
    "print('KNN Accuracy:', knn_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data for optimized models (replace with your actual accuracies)\n",
    "optimized_models = ['Random Forest', 'AdaBoost', 'Gradient Boost', 'SVM', 'KNN']\n",
    "optimized_accuracies = [1.00, 1.00, 1.00, 0.99, 1.00]  # Replace with actual optimized accuracies\n",
    "\n",
    "# Set up the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot Optimized Model Accuracies\n",
    "sns.barplot(x=optimized_accuracies, y=optimized_models, palette='Greens_d')\n",
    "plt.xlim(0.85, 1.0)  # Adjust limits as needed\n",
    "plt.title('Optimized Model Accuracies', fontsize=16)\n",
    "plt.xlabel('Accuracy', fontsize=14)\n",
    "plt.ylabel('Models', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "# Add accuracy values on the bars\n",
    "for i, v in enumerate(optimized_accuracies):\n",
    "    plt.text(v + 0.002, i, f\"{v:.2f}\", ha='center', fontsize=12)\n",
    "\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to fit into the figure area.\n",
    "plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data for baseline and optimized models (replace with your actual accuracies)\n",
    "baseline_models = ['Random Forest', 'AdaBoost', 'Gradient Boost', 'SVM', 'KNN']\n",
    "baseline_accuracies = [0.9750, 1.00, 0.9750, 0.9416, 0.933]  # Replace with actual baseline accuracies\n",
    "\n",
    "optimized_models = ['Random Forest', 'AdaBoost', 'Gradient Boost', 'SVM', 'KNN']\n",
    "optimized_accuracies = [1.00, 1.00, 1.00, 0.99, 1.00]  # Replace with actual optimized accuracies\n",
    "\n",
    "# Combine data into a DataFrame\n",
    "data = {\n",
    "    'Model': baseline_models + optimized_models,\n",
    "    'Accuracy': baseline_accuracies + optimized_accuracies,\n",
    "    'Type': ['Baseline'] * len(baseline_models) + ['Optimized'] * len(optimized_models)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set up the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting the comparison of model accuracies\n",
    "sns.barplot(x='Model', y='Accuracy', hue='Type', data=df, palette='viridis')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Comparison of Model Accuracies', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xlabel('Models', fontsize=14)\n",
    "plt.ylim(0.85, 1.05)  # Adjust limits as needed\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(title='Type', fontsize=12)\n",
    "\n",
    "# Add accuracy values on the bars\n",
    "for i, row in df.iterrows():\n",
    "    plt.text(i % len(baseline_models) + (0.15 if row['Type'] == 'Optimized' else -0.15), \n",
    "             row['Accuracy'] + 0.002, \n",
    "             f\"{row['Accuracy']:.2f}\", \n",
    "             ha='center', fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()  # Adjust layout to fit into the figure area.\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Function to plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    # Plot learning curves with error bars\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Baseline RandomForest model\n",
    "rf_baseline = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Plot Learning Curve for RandomForest Baseline Model\n",
    "plot_learning_curve(rf_baseline, \"Learning Curve (RandomForest Baseline Model)\", X, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[selected_features])\n",
    "\n",
    "# Optimized RandomForest model\n",
    "rf_optimized = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Plot Learning Curve for RandomForest Optimized Model\n",
    "plot_learning_curve(rf_optimized, \"Learning Curve (RandomForest Optimized Model)\", X_scaled, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Function to plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    # Plot learning curves with error bars\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# Baseline Gradient Boosting model\n",
    "gb_baseline = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Plot Learning Curve for Gradient Boosting Baseline Model\n",
    "plot_learning_curve(gb_baseline, \"Learning Curve (Gradient Boosting Baseline Model)\", X, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "### OPTIMIZED MODEL ###\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[selected_features])\n",
    "\n",
    "# Optimized Gradient Boosting model\n",
    "gb_optimized = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Plot Learning Curve for Gradient Boosting Optimized Model\n",
    "plot_learning_curve(gb_optimized, \"Learning Curve (Gradient Boosting Optimized Model)\", X_scaled, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Function to plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    # Plot learning curves with error bars\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# Baseline AdaBoost model\n",
    "ada_baseline = AdaBoostClassifier(n_estimators=50, learning_rate=0.5, random_state=42)\n",
    "\n",
    "# Plot Learning Curve for AdaBoost Baseline Model\n",
    "plot_learning_curve(ada_baseline, \"Learning Curve (AdaBoost Baseline Model)\", X, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "### OPTIMIZED MODEL ###\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[selected_features])\n",
    "\n",
    "# Optimized AdaBoost model\n",
    "ada_optimized = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Plot Learning Curve for AdaBoost Optimized Model\n",
    "plot_learning_curve(ada_optimized, \"Learning Curve (AdaBoost Optimized Model)\", X_scaled, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Function to plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    # Plot learning curves with error bars\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# Baseline SVM model (linear kernel)\n",
    "svm_baseline = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Plot Learning Curve for SVM Baseline Model\n",
    "plot_learning_curve(svm_baseline, \"Learning Curve (SVM Baseline Model)\", X, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "### OPTIMIZED MODEL ###\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[selected_features])\n",
    "\n",
    "# Optimized SVM model\n",
    "svm_optimized = SVC(random_state=42)\n",
    "\n",
    "# Plot Learning Curve for SVM Optimized Model\n",
    "plot_learning_curve(svm_optimized, \"Learning Curve (SVM Optimized Model)\", X_scaled, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "# X, y = ...  # Load your dataset here\n",
    "\n",
    "# Function to plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score (Accuracy)\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes\n",
    "    )\n",
    "    \n",
    "    # Compute mean and standard deviation for training and cross-validation scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    # Plot learning curves with error bars (standard deviation)\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.ylim(0, 1)  # Score (accuracy) ranges from 0 to 1\n",
    "    return plt\n",
    "\n",
    "### BASELINE MODEL ###\n",
    "\n",
    "# Split dataset for baseline model (optional if you've already split it)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
    "\n",
    "# Baseline SVM model (linear kernel)\n",
    "svm_baseline = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Plot Learning Curve for SVM Baseline Model\n",
    "plot_learning_curve(svm_baseline, \"Learning Curve (SVM Baseline Model)\", X, y, cv=5)\n",
    "plt.show()\n",
    "\n",
    "### OPTIMIZED MODEL ###\n",
    "\n",
    "# Feature selection and scaling for optimized model\n",
    "selected_features = ['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']  # Customize this according to your feature names\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X[selected_features])\n",
    "\n",
    "# Optimized SVM model\n",
    "svm_optimized = SVC(random_state=42)\n",
    "\n",
    "# Plot Learning Curve for SVM Optimized Model\n",
    "plot_learning_curve(svm_optimized, \"Learning Curve (SVM Optimized Model)\", X_scaled, y, cv=5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660dd522",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set the path to the Graphviz executable (if needed)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m graphviz\u001b[38;5;241m.\u001b[39mset_default_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "\n",
    "import graphviz\n",
    "\n",
    "# Set the path to the Graphviz executable (if needed)\n",
    "graphviz.set_default_engine('dot')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your dataset\n",
    "dataset = pd.read_csv(\"Kidney_data.csv\")\n",
    "\n",
    "# Preprocess your dataset\n",
    "# Assuming 'classification' is your target variable\n",
    "X = dataset[['sg', 'htn', 'hemo', 'dm', 'al', 'appet', 'rc', 'pc']]  # Adjust as needed\n",
    "y = dataset['classification']  # Adjust as needed\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing values for numerical columns using the mean\n",
    "imputer_num = SimpleImputer(strategy='mean')\n",
    "X[numerical_cols] = imputer_num.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Impute missing values for categorical columns using the most frequent value\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_cols] = imputer_cat.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Convert categorical variables to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[column] = le.fit_transform(X[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# If your target variable is also categorical, encode it\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Visualize one of the trees in the Random Forest\n",
    "# Export as dot file\n",
    "dot_data = export_graphviz(rf.estimators_[0], out_file=None, \n",
    "                           feature_names=X.columns,  \n",
    "                           class_names=np.unique(y).astype(str),  # Convert class names to string\n",
    "                           filled=True, rounded=True,  \n",
    "                           special_characters=True)  \n",
    "\n",
    "# Create a Graphviz source object\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"random_forest_tree\")  # Saves the tree as a PDF file\n",
    "graph.view()  # Opens the PDF file\n",
    "\n",
    "# To display the tree in a Jupyter notebook, uncomment the following line:\n",
    "# graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffa0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
